{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c14adb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Procesando ls1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:89: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s.astype(str), errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:273: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  baseline = ts[\"vlx_smooth\"].rolling(window=window_str, center=True, min_periods=1).median()\n",
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:275: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  mad = residual.abs().rolling(window=window_str, center=True, min_periods=1).median()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Procesando ls2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:89: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s.astype(str), errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:273: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  baseline = ts[\"vlx_smooth\"].rolling(window=window_str, center=True, min_periods=1).median()\n",
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:275: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  mad = residual.abs().rolling(window=window_str, center=True, min_periods=1).median()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Procesando ls3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:89: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s.astype(str), errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:273: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  baseline = ts[\"vlx_smooth\"].rolling(window=window_str, center=True, min_periods=1).median()\n",
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:275: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  mad = residual.abs().rolling(window=window_str, center=True, min_periods=1).median()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Procesando ls4 ...\n",
      "==> Procesando ls5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:89: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s.astype(str), errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:273: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  baseline = ts[\"vlx_smooth\"].rolling(window=window_str, center=True, min_periods=1).median()\n",
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:275: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  mad = residual.abs().rolling(window=window_str, center=True, min_periods=1).median()\n",
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:189: RuntimeWarning: All-NaN slice encountered\n",
      "  daily[\"max_drop_mm\"] = tmp[col].resample(\"1D\").apply(lambda s: np.nanmin(np.diff(s.values)) if len(s)>1 else np.nan)\n",
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:190: RuntimeWarning: All-NaN slice encountered\n",
      "  daily[\"max_rise_mm\"] = tmp[col].resample(\"1D\").apply(lambda s: np.nanmax(np.diff(s.values)) if len(s)>1 else np.nan)\n",
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:89: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s.astype(str), errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:273: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  baseline = ts[\"vlx_smooth\"].rolling(window=window_str, center=True, min_periods=1).median()\n",
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:275: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  mad = residual.abs().rolling(window=window_str, center=True, min_periods=1).median()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Procesando ls6 ...\n",
      "\n",
      "=== Resumen global ===\n",
      "  name                mac  sampling_s_median  coverage_h  n_points  \\\n",
      "0  ls1  40:22:D8:F1:E3:70             1802.0     527.331      1058   \n",
      "1  ls2  40:22:D8:F1:E2:CC             1802.0     527.324       902   \n",
      "2  ls3  40:22:D8:F1:E3:80             1802.0     526.952      1056   \n",
      "3  ls4  B8:D6:1A:60:95:30                1.0    1272.051       144   \n",
      "4  ls5  B8:D6:1A:60:94:1C             1802.0     527.784      1058   \n",
      "5  ls6  D8:13:2A:D2:36:B4              723.5     528.014        15   \n",
      "\n",
      "   n_occ_events  n_refill_events  occ_total_h  \\\n",
      "0             0                2          0.0   \n",
      "1             0                1          0.0   \n",
      "2             0                3          0.0   \n",
      "3             0                0          0.0   \n",
      "4             0                3          0.0   \n",
      "5             0                0          0.0   \n",
      "\n",
      "                                      plots_overview  \\\n",
      "0  D:\\ls_feed\\data_drive\\_analysis\\ls1\\overview_v...   \n",
      "1  D:\\ls_feed\\data_drive\\_analysis\\ls2\\overview_v...   \n",
      "2  D:\\ls_feed\\data_drive\\_analysis\\ls3\\overview_v...   \n",
      "3  D:\\ls_feed\\data_drive\\_analysis\\ls4\\overview_v...   \n",
      "4  D:\\ls_feed\\data_drive\\_analysis\\ls5\\overview_v...   \n",
      "5  D:\\ls_feed\\data_drive\\_analysis\\ls6\\overview_v...   \n",
      "\n",
      "                                          path_daily  \\\n",
      "0  D:\\ls_feed\\data_drive\\_analysis\\ls1\\daily_summ...   \n",
      "1  D:\\ls_feed\\data_drive\\_analysis\\ls2\\daily_summ...   \n",
      "2  D:\\ls_feed\\data_drive\\_analysis\\ls3\\daily_summ...   \n",
      "3  D:\\ls_feed\\data_drive\\_analysis\\ls4\\daily_summ...   \n",
      "4  D:\\ls_feed\\data_drive\\_analysis\\ls5\\daily_summ...   \n",
      "5  D:\\ls_feed\\data_drive\\_analysis\\ls6\\daily_summ...   \n",
      "\n",
      "                                       path_qc  \n",
      "0  D:\\ls_feed\\data_drive\\_analysis\\ls1\\qc.json  \n",
      "1  D:\\ls_feed\\data_drive\\_analysis\\ls2\\qc.json  \n",
      "2  D:\\ls_feed\\data_drive\\_analysis\\ls3\\qc.json  \n",
      "3  D:\\ls_feed\\data_drive\\_analysis\\ls4\\qc.json  \n",
      "4  D:\\ls_feed\\data_drive\\_analysis\\ls5\\qc.json  \n",
      "5  D:\\ls_feed\\data_drive\\_analysis\\ls6\\qc.json  \n",
      "\n",
      "Artefactos guardados en: D:\\ls_feed\\data_drive\\_analysis\n",
      " - Por comedero: <lsX>/events_*.csv, daily_summary.csv, qc.json, overview_vlx.png\n",
      " - Global: summary_all_feeders.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:89: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(s.astype(str), errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:273: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  baseline = ts[\"vlx_smooth\"].rolling(window=window_str, center=True, min_periods=1).median()\n",
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:275: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  mad = residual.abs().rolling(window=window_str, center=True, min_periods=1).median()\n",
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:189: RuntimeWarning: All-NaN slice encountered\n",
      "  daily[\"max_drop_mm\"] = tmp[col].resample(\"1D\").apply(lambda s: np.nanmin(np.diff(s.values)) if len(s)>1 else np.nan)\n",
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_23568\\1063222481.py:190: RuntimeWarning: All-NaN slice encountered\n",
      "  daily[\"max_rise_mm\"] = tmp[col].resample(\"1D\").apply(lambda s: np.nanmax(np.diff(s.values)) if len(s)>1 else np.nan)\n"
     ]
    }
   ],
   "source": [
    "# %% ANALÍTICA AVANZADA POR COMEDERO (LS1..LS6) A PARTIR DE PARQUETS \"split\"\n",
    "# Requisitos: pandas, numpy, matplotlib (instalados habitualmente con Anaconda)\n",
    "# Estructura de entrada esperada: data_drive/_parquet_export/split/ls1.parquet, ls2.parquet, ...\n",
    "# Columnas esperadas en cada parquet: mac (str), time (datetime/str/epoch), vlx (mm, numérico), source_file (str)\n",
    "#\n",
    "# Qué hace este notebook:\n",
    "# 1) Carga todos los .parquet \"split\" y valida esquema.\n",
    "# 2) Limpieza robusta: orden temporal, duplicados, outliers (Hampel), suavizado (mediana rodante).\n",
    "# 3) Métricas de calidad (QoS): frecuencia muestreo, lag, huecos, cobertura, valores planos, etc.\n",
    "# 4) Detección de eventos:\n",
    "#    - \"Ocupación\" (posible hocico del cerdo) por dips rápidos vs baseline (y alternativa por Otsu si hay bimodalidad).\n",
    "#    - \"Rellenos\" (refill) por saltos descendentes grandes en la distancia (si el sensor está sobre el nivel de pienso).\n",
    "# 5) Agregados diarios y resúmenes por comedero.\n",
    "# 6) Gráficas clave y export de artefactos en data_drive/_analysis/<lsX>/\n",
    "#\n",
    "# NOTA IMPORTANTE: sin calibración geométrica/peso, las inferencias de consumo son PROXIES (variaciones de distancia),\n",
    "# no kg reales. Si se conoce la geometría, puede añadirse una curva nivel->volumen/peso y el código contempla un hook.\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# --------------------------- CONFIGURACIÓN ---------------------------\n",
    "\n",
    "IN_DIR = Path(\"data_drive/_parquet_export/split\")\n",
    "OUT_ROOT = Path(\"data_drive/_analysis\")\n",
    "\n",
    "# Zona horaria y muestreo\n",
    "TIMEZONE = \"Europe/Madrid\"   # si tus timestamps son locales; si ya son UTC, cambia a \"UTC\"\n",
    "ASSUME_LOCALTIME = True      # si True, parsea 'time' como naive y lo localiza a TIMEZONE\n",
    "\n",
    "# Ventanas (se adaptan en minutos estimados a partir del muestreo real)\n",
    "ROLL_MED_MINUTES = 2.0       # mediana rodante de suavizado principal\n",
    "BASELINE_MINUTES = 30.0      # baseline lento para aislar dips (ocupación)\n",
    "Hampel_Window_Minutes = 3.0  # ventana para Hampel\n",
    "Hampel_k = 3.0               # sensibilidad Hampel (3-5 habitual)\n",
    "\n",
    "# Detección de eventos\n",
    "MIN_OCCUPANCY_SECONDS = 5.0      # duración mínima de un evento de ocupación\n",
    "GAP_MAX_SECONDS = 10.0           # máximo hueco entre puntos para considerar continuidad en eventos\n",
    "REFILL_JUMP_MM = None            # umbral fijo mm para refill; si None, se estima a -5*std(diff) o p1 neg\n",
    "REFILL_MIN_SEP_MIN = 10.0        # separación mínima entre recargas detectadas (anti-duplicados)\n",
    "BIMODAL_USE_OTSU_IF_TRUE = True  # si el histograma es claramente bimodal, usa Otsu como apoyo a dips\n",
    "\n",
    "# Export\n",
    "SAVE_PLOTS = True\n",
    "SAVE_EVENTS = True\n",
    "SAVE_DAILIES = True\n",
    "SAVE_QC = True\n",
    "\n",
    "# Conversión nivel->kg (hook). Si se conoce conversión, rellena esta función.\n",
    "def mm_to_kg(mm_delta: float) -> float:\n",
    "    # Placeholder: sin calibración, devolvemos 0.0. Sustituir según geometría del comedero.\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# --------------------------- UTILIDADES ---------------------------\n",
    "\n",
    "def ensure_columns(df: pd.DataFrame, required=(\"mac\",\"time\",\"vlx\")):\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Faltan columnas requeridas: {missing}. Presentes: {df.columns.tolist()}\")\n",
    "\n",
    "def parse_time_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convierte 'time' robustamente a datetime.\n",
    "    - Si es numérico grande puede ser epoch (s o ms).\n",
    "    - Si es string usa to_datetime(infer).\n",
    "    - Localiza a TIMEZONE si ASSUME_LOCALTIME=True y está naive.\n",
    "    \"\"\"\n",
    "    # Detectar tipo numérico con pandas, no con numpy (evita error con string[python])\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        m = s.median()\n",
    "        if m > 1e12:  # epoch en ms\n",
    "            dt = pd.to_datetime(s, unit=\"ms\", errors=\"coerce\")\n",
    "        elif m > 1e9:  # epoch en s\n",
    "            dt = pd.to_datetime(s, unit=\"s\", errors=\"coerce\")\n",
    "        else:\n",
    "            dt = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "    else:\n",
    "        dt = pd.to_datetime(s.astype(str), errors=\"coerce\", infer_datetime_format=True)\n",
    "\n",
    "    # Localización de zona horaria\n",
    "    if ASSUME_LOCALTIME:\n",
    "        if dt.dt.tz is None:\n",
    "            dt = dt.dt.tz_localize(TIMEZONE, nonexistent=\"NaT\", ambiguous=\"NaT\")\n",
    "        else:\n",
    "            dt = dt.dt.tz_convert(TIMEZONE)\n",
    "    return dt\n",
    "\n",
    "def hampel_filter(x: pd.Series, window: int, k: float = 3.0) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Filtro Hampel (mediana +- k*1.4826*MAD). Ventana mínima = 3.\n",
    "    \"\"\"\n",
    "    window = max(3, int(window))\n",
    "    med = x.rolling(window, center=True, min_periods=window).median()\n",
    "    mad = (x - med).abs().rolling(window, center=True, min_periods=window).median()\n",
    "    sigma = 1.4826 * mad\n",
    "    outliers = (x - med).abs() > (k * sigma)\n",
    "    x_filt = x.where(~outliers, med)\n",
    "    return x_filt.astype(float)\n",
    "\n",
    "\n",
    "def estimate_sampling_seconds(t: pd.Series) -> float:\n",
    "    dt = t.sort_values().diff().dropna().dt.total_seconds()\n",
    "    if dt.empty:\n",
    "        return 1.0\n",
    "    return float(dt.median())\n",
    "\n",
    "def minutes_to_points(minutes: float, sampling_s: float) -> int:\n",
    "    return max(1, int(round((minutes * 60.0) / max(0.5, sampling_s))))\n",
    "\n",
    "def otsu_threshold(values: np.ndarray, nbins: int = 256) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Umbral Otsu sobre valores 1D. Devuelve None si no hay dispersión.\n",
    "    \"\"\"\n",
    "    v = values[np.isfinite(values)]\n",
    "    if v.size < 10:\n",
    "        return None\n",
    "    if np.nanstd(v) < 1e-9:\n",
    "        return None\n",
    "    hist, bin_edges = np.histogram(v, bins=nbins)\n",
    "    weight1 = np.cumsum(hist)\n",
    "    weight2 = v.size - weight1\n",
    "    mean1 = np.cumsum(hist * ((bin_edges[:-1] + bin_edges[1:])/2))\n",
    "    mean2 = mean1[-1] - mean1\n",
    "    # evitar división por cero\n",
    "    valid = (weight1 > 0) & (weight2 > 0)\n",
    "    if not valid.any():\n",
    "        return None\n",
    "    variance12 = (mean1[valid] / weight1[valid] - mean2[valid] / weight2[valid]) ** 2 * (weight1[valid] * weight2[valid])\n",
    "    idx = np.argmax(variance12)\n",
    "    thr = ((bin_edges[:-1] + bin_edges[1:])/2)[np.where(valid)[0][idx]]\n",
    "    return float(thr)\n",
    "\n",
    "def label_runs(bool_series: pd.Series, time_index: pd.Series, gap_max_seconds: float, min_duration_seconds: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Etiqueta runs contiguos (True) como eventos. Rompe eventos cuando hay huecos > gap_max_seconds.\n",
    "    Devuelve DataFrame con start, end, duration_s.\n",
    "    \"\"\"\n",
    "    s = bool_series.fillna(False).astype(bool)\n",
    "    t = pd.to_datetime(time_index)\n",
    "\n",
    "    # Cortamos por huecos grandes\n",
    "    gaps = t.diff().dt.total_seconds().fillna(0.0)\n",
    "    # start de nuevo bloque si gap mayor a máximo o si cambia de False->True\n",
    "    block = (gaps > gap_max_seconds).cumsum()\n",
    "\n",
    "    # Dentro de cada bloque, detectamos runs True\n",
    "    events = []\n",
    "    for b, idx in s.groupby(block).groups.items():\n",
    "        sb = s.loc[idx]\n",
    "        tb = t.loc[idx]\n",
    "        change = sb.ne(sb.shift(1))\n",
    "        run_id = change.cumsum()\n",
    "        for rid, ids in sb.groupby(run_id).groups.items():\n",
    "            if not sb.loc[ids].iloc[0]:\n",
    "                continue\n",
    "            start = tb.loc[ids].iloc[0]\n",
    "            end = tb.loc[ids].iloc[-1]\n",
    "            dur = (end - start).total_seconds()\n",
    "            if dur >= min_duration_seconds:\n",
    "                events.append((start, end, dur))\n",
    "    if not events:\n",
    "        return pd.DataFrame(columns=[\"start\",\"end\",\"duration_s\"])\n",
    "    out = pd.DataFrame(events, columns=[\"start\",\"end\",\"duration_s\"])\n",
    "    return out\n",
    "\n",
    "def summarize_daily(df: pd.DataFrame, col: str, tz: str = TIMEZONE) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Agregados diarios de la columna 'col' y de sus diferencias (derivadas).\n",
    "    \"\"\"\n",
    "    tmp = df.set_index(\"time\").copy()\n",
    "    daily = pd.DataFrame()\n",
    "    daily[\"n_points\"] = tmp[col].resample(\"1D\").size()\n",
    "    daily[\"vlx_mm_med\"] = tmp[col].resample(\"1D\").median()\n",
    "    daily[\"vlx_mm_mean\"] = tmp[col].resample(\"1D\").mean()\n",
    "    daily[\"vlx_mm_std\"] = tmp[col].resample(\"1D\").std()\n",
    "    # drift (consumo proxy, depende de montaje; signo orientativo)\n",
    "    daily[\"net_diff_mm\"] = tmp[col].resample(\"1D\").apply(lambda s: float(s.iloc[-1] - s.iloc[0]) if len(s)>1 else np.nan)\n",
    "    daily[\"max_drop_mm\"] = tmp[col].resample(\"1D\").apply(lambda s: np.nanmin(np.diff(s.values)) if len(s)>1 else np.nan)\n",
    "    daily[\"max_rise_mm\"] = tmp[col].resample(\"1D\").apply(lambda s: np.nanmax(np.diff(s.values)) if len(s)>1 else np.nan)\n",
    "    # conversión a kg (si hay calibración)\n",
    "    daily[\"net_delta_kg\"] = daily[\"net_diff_mm\"].apply(mm_to_kg)\n",
    "    return daily\n",
    "\n",
    "def quality_metrics(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Métricas de calidad/operación del sensor.\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        return {\"n\": 0}\n",
    "    sampling_s = estimate_sampling_seconds(df[\"time\"])\n",
    "    coverage_h = (df[\"time\"].max() - df[\"time\"].min()).total_seconds() / 3600.0\n",
    "    duplicates = int(df.duplicated(subset=[\"time\"]).sum())\n",
    "    # Huecos\n",
    "    gaps_s = df[\"time\"].sort_values().diff().dt.total_seconds().dropna()\n",
    "    big_gaps = int((gaps_s > GAP_MAX_SECONDS).sum())\n",
    "    pct_missing_time = float((gaps_s[gaps_s > GAP_MAX_SECONDS].sum()) / (coverage_h * 3600.0 + 1e-9))\n",
    "    # Valores planos prolongados\n",
    "    flat_runs = (df[\"vlx_smooth\"].round(3).diff().abs() < 1e-6).astype(int)\n",
    "    flat_len = flat_runs.groupby((flat_runs!=flat_runs.shift()).cumsum()).transform('size')\n",
    "    long_flat = int((flat_len >= minutes_to_points(5.0, sampling_s)).sum())\n",
    "    return {\n",
    "        \"n_points\": int(n),\n",
    "        \"time_start\": str(df[\"time\"].min()),\n",
    "        \"time_end\": str(df[\"time\"].max()),\n",
    "        \"coverage_hours\": round(coverage_h, 3),\n",
    "        \"sampling_seconds_median\": round(sampling_s, 3),\n",
    "        \"duplicates_by_time\": duplicates,\n",
    "        \"num_gaps_gt_threshold\": big_gaps,\n",
    "        \"pct_time_missing_est\": round(100*pct_missing_time, 3),\n",
    "        \"flat_values_long_count\": long_flat,\n",
    "        \"vlx_mm_min\": float(df[\"vlx_mm\"].min()),\n",
    "        \"vlx_mm_max\": float(df[\"vlx_mm\"].max()),\n",
    "    }\n",
    "\n",
    "def plot_overview(df: pd.DataFrame, name: str, out_dir: Path,\n",
    "                  events_occ: Optional[pd.DataFrame], events_refill: Optional[pd.DataFrame]):\n",
    "    if not SAVE_PLOTS:\n",
    "        return\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(df[\"time\"], df[\"vlx_smooth\"], label=\"vlx_smooth (mm)\")\n",
    "    ax.set_title(f\"{name} - Distancia suavizada (mm)\")\n",
    "    ax.set_xlabel(\"Tiempo\")\n",
    "    ax.set_ylabel(\"mm (menor = más cerca)\")\n",
    "    # Marca eventos\n",
    "    if events_occ is not None and len(events_occ) > 0:\n",
    "        for _, r in events_occ.iterrows():\n",
    "            ax.axvspan(r[\"start\"], r[\"end\"], alpha=0.15, label=\"ocupación\" if _==0 else None, color=\"tab:orange\")\n",
    "    if events_refill is not None and len(events_refill) > 0:\n",
    "        for _, r in events_refill.iterrows():\n",
    "            ax.axvline(r[\"time\"], alpha=0.4, label=\"refill\" if _==0 else None)\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    fig.tight_layout()\n",
    "    p = out_dir / \"overview_vlx.png\"\n",
    "    fig.savefig(p, dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "def _smooth_vlx(df: pd.DataFrame, sampling_s: float) -> pd.Series:\n",
    "    from math import ceil\n",
    "    # calcula tamaño de ventana por puntos pero con mínimo 3\n",
    "    w_med = max(3, int(round((ROLL_MED_MINUTES * 60.0) / max(0.5, sampling_s))))\n",
    "    return (\n",
    "        df[\"vlx_filt\"]\n",
    "        .rolling(w_med, center=True, min_periods=w_med)\n",
    "        .median()\n",
    "    )\n",
    "\n",
    "\n",
    "def detect_occupancy(df: pd.DataFrame, sampling_s: float):\n",
    "    \"\"\"\n",
    "    Calcula baseline lento con ventana temporal 'BASELINE_MINUTES' y marca dips significativos.\n",
    "    Devuelve (serie_booleana, umbral_otsu_opcional).\n",
    "    \"\"\"\n",
    "    # Aseguramos que vlx_smooth existe; si no, lo generamos con el helper\n",
    "    if \"vlx_smooth\" not in df.columns or df[\"vlx_smooth\"].isna().all():\n",
    "        df[\"vlx_smooth\"] = _smooth_vlx(df, sampling_s)\n",
    "\n",
    "    ts = df.set_index(\"time\").sort_index()\n",
    "    window_str = f\"{max(1, int(round(BASELINE_MINUTES)))}T\"  # ej. \"30T\"\n",
    "    # baseline y sigma robusta con min_periods=1 para evitar errores\n",
    "    baseline = ts[\"vlx_smooth\"].rolling(window=window_str, center=True, min_periods=1).median()\n",
    "    residual = ts[\"vlx_smooth\"] - baseline\n",
    "    mad = residual.abs().rolling(window=window_str, center=True, min_periods=1).median()\n",
    "    sigma = 1.4826 * mad\n",
    "    z = residual / sigma.mask(sigma == 0, np.nan)\n",
    "    occ_dips = (z < -2.5).fillna(False)  # booleano por índice temporal\n",
    "\n",
    "    # Opción Otsu si hay bimodalidad\n",
    "    otsu_thr = None\n",
    "    if BIMODAL_USE_OTSU_IF_TRUE:\n",
    "        thr = otsu_threshold(df[\"vlx_smooth\"].to_numpy())\n",
    "        if thr is not None and np.isfinite(thr):\n",
    "            otsu_thr = float(thr)\n",
    "            occ_dips = occ_dips | (ts[\"vlx_smooth\"] < thr)\n",
    "\n",
    "    # Devolvemos alineado a df (mismo orden/longitud)\n",
    "    occ_bool = pd.Series(occ_dips.values, index=df.index)\n",
    "    return occ_bool, otsu_thr\n",
    "\n",
    "def detect_refills(df: pd.DataFrame, sampling_s: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detecta \"refills\" como saltos descendentes pronunciados en vlx_smooth (más pienso -> menor distancia).\n",
    "    Umbral automático si REFILL_JUMP_MM es None.\n",
    "    Devuelve DataFrame con time y jump_mm (negativo).\n",
    "    \"\"\"\n",
    "    d = df[\"vlx_smooth\"].diff()\n",
    "    neg = d.dropna()[d.dropna() < 0.0]\n",
    "    if REFILL_JUMP_MM is None:\n",
    "        if len(neg) == 0:\n",
    "            thr = -np.inf\n",
    "        else:\n",
    "            thr = min(neg.quantile(0.01), -5.0 * float(d.std() or 0.0))  # muy conservador\n",
    "            # aseguramos que sea negativo y con magnitud relevante\n",
    "            thr = min(thr, - np.nanpercentile(abs(d.values), 75))\n",
    "    else:\n",
    "        thr = -abs(REFILL_JUMP_MM)\n",
    "\n",
    "    cand = d[d <= thr]\n",
    "    if cand.empty:\n",
    "        return pd.DataFrame(columns=[\"time\",\"jump_mm\"])\n",
    "\n",
    "    # anti-duplicados: separar en el tiempo\n",
    "    min_sep_points = minutes_to_points(REFILL_MIN_SEP_MIN, sampling_s)\n",
    "    idx = cand.index\n",
    "    keep = []\n",
    "    last_i = None\n",
    "    for i in idx:\n",
    "        if last_i is None or (i - last_i) >= min_sep_points:\n",
    "            keep.append(i)\n",
    "            last_i = i\n",
    "    events = pd.DataFrame({\n",
    "        \"time\": df.loc[keep, \"time\"].values,\n",
    "        \"jump_mm\": d.loc[keep].values\n",
    "    })\n",
    "    return events\n",
    "\n",
    "def process_one(df_in: pd.DataFrame, name: str, out_dir: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Pipeline por comedero.\n",
    "    \"\"\"\n",
    "    ensure_columns(df_in)\n",
    "    df = df_in.copy()\n",
    "\n",
    "    # Tipos\n",
    "    df[\"mac\"] = df[\"mac\"].astype(\"string\").str.strip()\n",
    "    df[\"time\"] = parse_time_series(df[\"time\"])\n",
    "    df[\"vlx_mm\"] = pd.to_numeric(df[\"vlx\"], errors=\"coerce\")\n",
    "    # source opcional\n",
    "    if \"source_file\" in df.columns:\n",
    "        df[\"source_file\"] = df[\"source_file\"].astype(\"string\")\n",
    "    # Limpieza básica\n",
    "    df = df.dropna(subset=[\"time\"]).sort_values(\"time\").drop_duplicates(subset=[\"time\"], keep=\"last\")\n",
    "    sampling_s = estimate_sampling_seconds(df[\"time\"])\n",
    "    if len(df) == 0:\n",
    "        return {\"name\": name, \"error\": \"sin datos tras limpieza\"}\n",
    "\n",
    "    # Hampel -> suavizado\n",
    "        # Hampel -> suavizado\n",
    "    w_hampel = minutes_to_points(Hampel_Window_Minutes, sampling_s)\n",
    "    w_hampel = max(3, w_hampel)\n",
    "    df[\"vlx_filt\"] = hampel_filter(df[\"vlx_mm\"], window=w_hampel, k=Hampel_k)\n",
    "\n",
    "    # Mediana rodante principal\n",
    "    w_med = minutes_to_points(ROLL_MED_MINUTES, sampling_s)\n",
    "    w_med = max(3, w_med)\n",
    "    df[\"vlx_smooth\"] = (\n",
    "        df[\"vlx_filt\"]\n",
    "        .rolling(w_med, center=True, min_periods=w_med)\n",
    "        .median()\n",
    "    )\n",
    "\n",
    "    # Métricas de calidad\n",
    "    qc = quality_metrics(df)\n",
    "\n",
    "    # Ocupación\n",
    "    occ_bool, otsu_thr = detect_occupancy(df, sampling_s)\n",
    "    events_occ = label_runs(occ_bool, df[\"time\"], gap_max_seconds=GAP_MAX_SECONDS, min_duration_seconds=MIN_OCCUPANCY_SECONDS)\n",
    "\n",
    "    # Refill\n",
    "    events_refill = detect_refills(df, sampling_s)\n",
    "\n",
    "    # Agregados diarios\n",
    "    daily = summarize_daily(df, \"vlx_smooth\")\n",
    "\n",
    "    # Export\n",
    "    com_dir = out_dir / name\n",
    "    com_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if SAVE_EVENTS:\n",
    "        events_occ.to_csv(com_dir / \"events_occupancy.csv\", index=False)\n",
    "        events_refill.to_csv(com_dir / \"events_refill.csv\", index=False)\n",
    "    if SAVE_DAILIES:\n",
    "        daily.to_csv(com_dir / \"daily_summary.csv\", index=True)\n",
    "    if SAVE_QC:\n",
    "        qc_out = qc.copy()\n",
    "        if otsu_thr is not None:\n",
    "            qc_out[\"otsu_threshold_mm\"] = float(otsu_thr)\n",
    "        (com_dir / \"qc.json\").write_text(json.dumps(qc_out, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    # Plot\n",
    "    plot_overview(df, name, com_dir, events_occ, events_refill)\n",
    "\n",
    "    # KPIs de ocupación (tiempo total/día)\n",
    "    if len(events_occ) > 0:\n",
    "        occ_df = events_occ.copy()\n",
    "        occ_df[\"date\"] = occ_df[\"start\"].dt.tz_localize(TIMEZONE).dt.date\n",
    "        occ_daily = occ_df.groupby(\"date\")[\"duration_s\"].sum().rename(\"occ_seconds_day\")\n",
    "        occ_daily.to_csv(com_dir / \"occupancy_daily_seconds.csv\", index=True)\n",
    "    else:\n",
    "        occ_daily = pd.Series(dtype=float)\n",
    "\n",
    "    # KPI recargas por día\n",
    "    if len(events_refill) > 0:\n",
    "        re_df = events_refill.copy()\n",
    "        re_df[\"date\"] = pd.to_datetime(re_df[\"time\"]).dt.tz_localize(TIMEZONE).dt.date\n",
    "        refill_daily = re_df.groupby(\"date\")[\"jump_mm\"].count().rename(\"refill_count_day\")\n",
    "        refill_daily.to_csv(com_dir / \"refill_daily_count.csv\", index=True)\n",
    "    else:\n",
    "        refill_daily = pd.Series(dtype=float)\n",
    "\n",
    "    # Resultado resumen\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"mac\": df[\"mac\"].mode().iloc[0] if not df[\"mac\"].isna().all() else \"\",\n",
    "        \"sampling_s_median\": qc[\"sampling_seconds_median\"],\n",
    "        \"coverage_h\": qc[\"coverage_hours\"],\n",
    "        \"n_points\": qc[\"n_points\"],\n",
    "        \"n_occ_events\": int(len(events_occ)),\n",
    "        \"n_refill_events\": int(len(events_refill)),\n",
    "        \"occ_total_h\": float(events_occ[\"duration_s\"].sum()/3600.0) if len(events_occ)>0 else 0.0,\n",
    "        \"plots_overview\": str((out_dir / name / \"overview_vlx.png\").resolve()) if SAVE_PLOTS else \"\",\n",
    "        \"path_daily\": str((out_dir / name / \"daily_summary.csv\").resolve()) if SAVE_DAILIES else \"\",\n",
    "        \"path_qc\": str((out_dir / name / \"qc.json\").resolve()) if SAVE_QC else \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "# --------------------------- CARGA Y EJECUCIÓN ---------------------------\n",
    "\n",
    "parquets = sorted(IN_DIR.glob(\"*.parquet\"))\n",
    "if not parquets:\n",
    "    raise SystemExit(f\"No se encontraron .parquet en {IN_DIR.resolve()}\")\n",
    "\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dfs: Dict[str, pd.DataFrame] = {}\n",
    "for p in parquets:\n",
    "    name = p.stem.lower()  # ej: ls1\n",
    "    df = pd.read_parquet(p)\n",
    "    # Validación mínima y casting\n",
    "    ensure_columns(df, (\"mac\",\"time\",\"vlx\"))\n",
    "    dfs[name] = df[[\"mac\",\"time\",\"vlx\"] + ([c for c in df.columns if c==\"source_file\"])]\n",
    "\n",
    "# Procesa cada comedero\n",
    "summaries = []\n",
    "for name, df in dfs.items():\n",
    "    print(f\"==> Procesando {name} ...\")\n",
    "    res = process_one(df, name=name, out_dir=OUT_ROOT)\n",
    "    summaries.append(res)\n",
    "\n",
    "# Resumen global\n",
    "summary_df = pd.DataFrame(summaries).sort_values(\"name\")\n",
    "summary_df.to_csv(OUT_ROOT / \"summary_all_feeders.csv\", index=False)\n",
    "print(\"\\n=== Resumen global ===\")\n",
    "print(summary_df)\n",
    "\n",
    "# Ayuda adicional: mostrar dónde está todo\n",
    "print(f\"\\nArtefactos guardados en: {OUT_ROOT.resolve()}\")\n",
    "print(\" - Por comedero: <lsX>/events_*.csv, daily_summary.csv, qc.json, overview_vlx.png\")\n",
    "print(\" - Global: summary_all_feeders.csv\")\n",
    "\n",
    "\n",
    "# --------------------------- NOTAS DE INTERPRETACIÓN ---------------------------\n",
    "# - \"vlx_smooth\" es la distancia suavizada. Si el sensor está encima del pienso:\n",
    "#     * Distancias MENORES -> más pienso (nivel alto) o intrusión (hocico).\n",
    "#     * Distancias MAYORES -> menos pienso (nivel bajo).\n",
    "# - Ocupación: se estima como episodios de dips rápidos (z<-2.5 respecto a baseline lento) y/o por Otsu si bimodalidad.\n",
    "# - Refill: saltos negativos grandes (distancia cae bruscamente) y espaciados ≥ REFILL_MIN_SEP_MIN.\n",
    "# - Daily net_diff_mm: diferencia final-inicial del día (signo depende del montaje).\n",
    "#   Para consumo real, sustituir mm_to_kg() con la conversión geométrica adecuada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9035ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
